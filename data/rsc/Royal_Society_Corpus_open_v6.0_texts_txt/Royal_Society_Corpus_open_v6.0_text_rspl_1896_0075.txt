]\gt ; On Bravais ' in case of Correl .
4 On the Significance of for ion .
in the case of Skew elation.\ldquo ; By G. Communicated by Professor KARL PEARS Received December 14 , \mdash ; Read uary 1 The only theory of correlation at available for use is based on the normal law of frequency , but , , this law is not valid in a great many cases which are both and impol.tant .
It does not good , to take examples from for statistics of fertility in man , for meas uremen b on , or weight measul.ements even on adults .
In economic tics , on tl1c other hand , normal ibutions ( ppear to be xcepti o : variation of wages , prices , valuations , pauperism , and so always skew .
In cases like these we have at no nlGftlS of ' measuring the COl.relation by one or more " " ' such as are afforded by the normal theory .
It seems worth while uoting , under circumstances , that in ordinary practice statisticians never concern themselves with the of the correlation , nol.mal or otherwise , but yet obtain ults of interest\mdash ; though in numerical ctness nnd fre quently in ccrtainty .
Suppose the case to be one in which two variables are varying ether iu time , curves are of these appear , speaking , to and fall together , the vbles are to ) Iated .
If on the other hand it is not a of associated may ) tabulated iu to the magoitude of one variable , and tlJen it may be seen the entries of other variable also in .
Both nlethodsttl.e of very rougl ] , and will only vcrv close tion , but they contain , it seems to me , the point of prime tance at all events with regard to economic statistics .
In ftll the cl lbsicul examples of statistical corl.elation ( e.g. , corn prices and vagrancy , out-relief and es ) only imarily concerned with the question is a ith a large or bnlall the question as to of this eiSociation and the elative frequency of of the bless is , at any rate on a first ation , of ) secondarvv ] ) ortance .
Let , Oy be the axes of a three the mean of the stlrface of , and let the points marked the of successive -arrays , .
on some curve that may be called the of regl'essio of on ow let a line , , be tted to subjecting the distances of the mealls from the line to some minimal condition .
If the slope of RR is we may say that large values of are on the whole associated with values of , if it is negative values of are associated with small values of urther , if the slope of RR to lihe vertical be given we shall have a measure of a practical kind of the shift of the meatl of an -array when its type is altered .
The equation to RR quently gives a concise and detinite to two most important statistical questionls .
It is also evident that if the means of the arrays actually lie in a straight line ( as in normal correlation ) , the equation to RR must be the equation to the line of ression .
Let be the number of obselwations in any -array , and let be the horizontal distance of the mean of this from the line propose to subject ) line to the condition that the sum of all quantities lik-o shall 'oe a minilnum , i.e. , I shall use the condition of least squares .
I do this soleiy for convenience of analysis ; I do nob claim for the method adopted any peculiar advantage as regards probability of its results .
It would , in fact , be absurd to do so , for I am postulating at the very outset that the curve of regression is only exceptionally a line ; there can consequently be no meaning in soeking for the most probable line represent regression .
Let be a pair of associated deviations , let be the stnndard deviation of any array about its own mean , and let for ression , , in the case of Ske Correlalion .
be the equation to RR .
Then for any one array Hence , extending the .
of to tion ovel the whole face But in this expression is independent of and , it , in fac a characteristic of the surface .
, making a is equivalent to making a minimum .
That is to say , we may rsgard our method in , er light .
We may that we form a -valned relation etwcen a pair of associated deviations , such that the sum of squal.es of our errors in imating nny one its by the relation is a minimunl .
This single-}-alued ] ation , which the characteristic relntion , is simply the to the line of regression .
Therc will be two such equations to be formed corresponding to lines of ession .
The of the ntethod at orlce be extended to the case of correlation betwesn everal viables ( , \amp ; c. Let be the nnml ) of observations in an of associated with fixed values , of the inbles , Jet bc the deviation of this array , let be the difference of its mean from the given by a ression e Tlleu , as before , we shall the coefficients , 0i13 , \amp ; c. , so to Snd a .
But this is equivftleut tc minimum for Hence , we may say that we solve a single-valued between our val'iables ; the tion being that the SU111 of the squares of the made in its ltud dues x \amp ; c. , is the least poshible .
Jn of normnl correla480 Mr. .
U. Yule .
On the Significance of Bravais ' Formuloe tion this " " characteristic relation ' must become the " " equation regression " " -bich gives the means of any -array , as in this way can be made a minimum , i.e. , zero .
It be said that it would be more natural to form a " " characteristic elation \ldquo ; between the absolute values of the variables and not their deviations the mean .
This may , however , be most conveniently done by with the mean as origin until the acteristic is obtained , and then transferring the equation to zero as origin .
It would be much more laborious and would only lead to the same result if zero were used as origin .
We may now to the discussion of the special cases of two , three , or more variables .
The actual formulae obtained are not , it be found , novel in themselves , but throw an unexpected light on the .
of the expressions previously given by Bravais*for the case of normal correlation .
( 1 ) Case of Two .\mdash ; Since and represent deviations their respective means , we have , to denote summation ovel the whole .
sulface , The istic o regression equations which we have to find are of the form ( 1 ) .
Taking the equation for , the normal equations for and are . . . . . . . . . . . . . .
( 2 ) , being the number of correlated pairs .
From the of { hese equations we have at once From the second ( xy ) To our tion let us write and are then the two standard-deviabions or errol .
S of mean * ' ' moil .
divers 1846 , p. 265 , and on " " Reglession , Heredity , \amp ; c , vol. 187 ( 1896 ) , p. 261 et seq. for Regression , , in case of Skew Correlation .
481 square .
is Bravais ' of the coefficisnt of correlation .
Rewriting in terms of these symbols , we have . . . . . . . . . . . . . . . . . . . .
( 3 ) .
Similarly , . . . . . . . . . . . . . .
( 4 ) .
But the expressions on the right of ( 3 ) and ( 4 ) are the values obtained by Bravais on the assumption of normal correlation for the regression of on , and the regression of y.on .
That is to say , the Bravais values for the ressions are lmply those values and , which make and respectively minima , whatever be the form of the correlation between the tu variables .
Agaiu , ever the form of the correlation , if the ression be really linear , the equations to the lines of regression given above ( as we pointed out in the introduction ) .
This theorem admits of a very ] and direct geometrical proof .
Let be the number of correlated pairs in any one array taken paraUel to the axis of , and let be the angle that the line of egression makes with the axis of .
Then , for a } , or extending the significance of to summation over the wholc surface , tall that is , In any case , then , the appears to be linear , ' formulae may be used at once u:ithout troubling to invesfigaie of the distrib ution .
The exponential character of the surface appears to have nothing whatever to do with the result .
To return , again , to the most general case , we see that both coefficients of regression must have the same \ldquo ; namely , the sign of .
Hence , either regression will serve to indicate whether there is corl'elation or no , for there is no reason , a priori , why the values of and , as determined above , should be positive rather than negative .
But , nevertheless , the ressions are not convenient measures of correlation , for , ou comparing two similar cases , we may find , say , 482 .
G. U. Yule .
On the of Bravais ' Formuloe where are the regressions in the two cases .
To which distribution are we , in such a case , to attribute the greater correlation P Bravais ' coefficient solves the difficulty , we may say , in one way , by taking the netrical mean of the two regressions as the measure of correlation .
It will still remain valid for non-normal correlation .
But there are other and less arbitrary interpretations even iu the general case .
Suppose that instead of measuring and in arbitrary units we measure each in terms of its own standard deviation .
Then let us vrite , and solve for by .
the method of least squares .
We have omitted a constant on the right-hand side , since it would vanish as before .
We have , at once , .
( 6 ) , That is to say , if we measure and each in terms of its own standard deviation , becomes at once the regression of on , and the regression of on .
The regressions being , in fact , the fundamental physical quantities , is , coefficient of correlation because it is a coefficient of regression .
* Again , let us form the sums of the squares of residuals in equations and ( 5 ) .
Inserting the values of , and , we have\mdash ; Any one of these quantities , bein .
the sum of a series of squares , must be positive .
Hence cannot be greater than unity .
If be equal to unity , or if the correlation be perfect , all the above three sums become zero .
But can only vanish if in every case , or if the relation hold good , * That the regression becomes the coefflcient of correlation then each deviation is measured in terms of its standard-deviation in the case of normal correlation has been pointed outby Mr. Francis Galton .
Pearso Phil. Trans p. 307 , note .
for Regression , , in the case of Skew Correlation .
the sign of the last term depending on the sign of .
Hence the statement that two variables are " " perfectly correlated\ldquo ; implies that relation ( 8 ) olds good , or that au pairs of deviations bear the same ratio to one .
It follows that in correlation , where the means of arrays are not collinear , or the deviation of the mean of the array is not a linear function of the deviation of the type , can never be unity , though we know from expcrience that it can approach pretty closely to that value .
If the regression be very far from linear , some caution must evidently be used in employing to compare two different distribu ions .
In the case of normal correlation , is the standard deviation of any array of the variables , corresponding to a single type of is similarly the standard deviation of any array of the variables , corresponding to a single type of .
In the general case , the firs expression may be interpreted as the mean standard deviation of the -arrays from line of regression , and the second expression as the mean standard deviation of the -arrays from the line of regression .
Otherwise we may regard as the standard error made in estimating from the relation and as the standard error made in estimating from the relation these interpretations being independent of the form of tion .
( 2 .
) Case of Three Variables .
Let the three corl.elated varia , bless be , and let , :t'3 denote deviations of these variables from their respective means .
us write , for brevity , Mr. G. U. Yule .
On the Significance of Bravais ' Formuloe Our stic or regression-equation will now be of the form ( 9 ) , and being the unknowns to be determined from the observations by the method of least squares .
I have omitted a constant term on the right-hand side , since its least-square value would be zero as before .
The two normal equations are now\mdash ; , or replacing the sums by the symbols defined above , and simplify(10 ) , wh nce ( ll ) .
That is , the characteristic relation between and is\mdash ; .
( 12 ) .
Now Bravais showed that if the correlahon were normal , and we selected a group or array of 's with regard to special values and of and , then being the deviation of the mean of the selected 's from the -mean of the whole material , where and have the values given in ( 11 ) .
But evidently the relation is of much greater generality ; it holds good so long as is a linear function of and , whauever be the law of frequency .
Further , the values of and above determined , are , under any circumstances , such that is a minimum .
If we insert in this expression the values of and from ( 11 ) , we have , after some reduction , .
( 13 ) , for Regression , , in the case of Skew Correlatio .
485 In correlation is the standard deviation of an array , responding to any given types of and relation it may be regarded as the mean standard deviation of the -arrays from the plane or as the standard made in estimating from and by relation ( 12 ) .
The quantity is of some interest , as ib exactly takes the place of in the residual expressions ( 7 ) .
may , in fact , be regarded as a coefficient of correlation between and ; it can only bs unity if the liuear ] ation ( or ( 12 ) hold good in case .
The quantities , \amp ; c. ( the others may be written down by symmetry ) , be termed the net regressions of on on \amp ; c. If we write 2 for 1 and 1 for 2 in the value of , we have being the the net regression of on .
In normal correlation , and the regressions for any roup of or 's associated with a fixed type of .
Honce , in this case ( normal correlation ) , the coefficient of correlation for such a group is the geometrical mean of the two regressions , or a quantity that may be called the net coefficient of correlation between and The similar net coefficients between and , may be written down by suffixes .
In normal correlation is quite stl'ictly the coefficient of correlation for any sub-group of 's and atever the associated type of .
In generalised correlation this will not be so , can only retain an average The method does not appear to be capable of investigating nges net coefficient as we from one type another , but it may be noted bhat whatever the form of he correlation , retains three of the chief properties of the ordinary coefficients : ( 1 ) it can only be * My quantities , , \amp ; c. , wele termed by Profe , sor Pearson \amp ; c ' Phil. Tr , vol. 187 ( 1896 ) , p. 287 ) , " " Coefficients of double ression , \ldquo ; and quantities like , \amp ; c. , " " coefficients of double correlation My quantities hc did not llse .
Having mecl the ' net correlation it seemed most natural to l.ename the 's " " net regressions the 's and 's the Col.espon ing quantities .
Some of my results given above were quoted by Professol Pearson in his paper notes .
486 Mr. G. U. yule .
On the iicance of Bravais ' Formulce zero if both net regressions are zero ; ( 2 ) it is a symmetrical function of the variables ; ( 3 ) it cannot be greater than unity ; for , by or adding to both sides , and to the right-hand side .
If any two coefficients , say , be supposed known , the inequality we have used above will give us limits for the valne of the third .
Throwing it into the form we have must lie between the limits The values of .
these limits for some special cases are collected iu the following table:\mdash ; One is rather pronc to argue that if A be correlated with , and with , A will be correlated with C. Evidently this is not necessary .
A may be correlated with , and positively correlated with , but yet A may , in general , be negatively correlated with C. Only , if the coefficients ( AB ) and ( BC ) are both numericalIy greater than 0 , can one even ascribe the correct sign to the corre .
lation .
It is evident that one would , in general , expect to make a smaller standard error in estimating from the two associated variables and , than in estimating it from one only , say .
But it seems desirable to prove this specifically , and to investigate under what conditious it wiIl hold good .
The necessary condition is\mdash ; for Regression , , in case of Skew Correlation .
487 that is , or But is the numerator of , the net coefficient of corre lation between and .
Hence the standard error in the second case will be always less than in the first , so long as is not zero .
The condition is somewhat interesting .
To take an arithmetical example , suppose one had in some actual case One might very naturally imagine that the ion of the variable with a fairly high corr.elation coefficient would cunsiderably lessen the standard deviation of the array ; but this is not so , for so the third variable would be of no assistance . .
Cnse of Four Variables .
This case is , haps , of sufficient practical importance to arrant our developing the results at length in the last .
If , be the associated deviations of the four variables from their espective means , the characteristic will } of the form .
( 14 ) .
The normal equations for the 's are , in our notation , and so on for the others , , \amp ; c. , we may call the net regressions of on on , \amp ; c. , as before .
By parity of notation , have and we may call the net coefficient of correlation between and .
Expandin the determinants , we have , in fact , ( 16 ) .
There are six such net coefficients , .
The above values of the regressions are again thoss usually obtained on the assumption of normal correlation .
* The net correlation becomcs , on that assumption , ) coefficient of correlation for any gloup of the variables associated with flxed types of ili3 and If we write , we have , after some rather lengthy rednction , , where In normal correlation , is the standard deviation of all arrays associated with fixed types of , and .
In general correlation , it is most easily interpreted as the standard error made iu estimating , by equation ( 14 ) , from its associated values of and As in the case of three val.iables , the quantity may be considered as a coefficient of elation .
It can ran betfveen , and can only become unity if the linear relation ( 14 ) hold good in eacb individual instance .
We showed at the end of the Iast section that the standard error made in iimating x from the relation *Professo Pearson , " " Begl.ession , , and Panmixia .
Phil : Trans , vol. 187 1896 ) , p. 294 .
Jfathematical Contributions to the Th of Evotution .
48 was always less than the standard error when only was taken into account , unless We may now prove the similar theorem that when we use three variables , , on which to base the estimate , the standard error will be again decreased , unless The condition that , in present case , shall be less than in the last , is , in fact , .
This may be finally reduced to\mdash ; that is The nent of the general case of variables , so far as regat .
obtaining the , is obvious , and ib is unuecessary to it at length .
We can now see that the use of normal ssion formulae is quite in all cases , so long as the limitations of interpretation are nised .
Bravais ' always remains a of correlation .
These results I must plead as } for my use of normal formulae in here the ation was markedly non-normal .
Iathematical Cutions to the of tion .
a F'orm of Spurious ] which may Indices are used in ) ebIeasul.enlent of\ldquo ; KARL sity C , London .
celved 1)eccmber29 , -Read , 1897 .
( 1 ) If the ratio of two absolute sure ments on the same or different ox.gans be taken it is convenient to t , erm this ratio an If and be two functions of the three variabies , and these variables be selected at random so that there exists no correlation between , or will still be found to * Economic Journal , ' ) , lS95 .
and .
Dec. , lS96 ; ' On Correlation of Total Pauperism Proportion of Out-relief.\ldquo ;

